\documentclass[12pt, a4paper]{article}
\usepackage{sectsty}
\usepackage{amsmath}
\usepackage{enumitem}
\usepackage{nicefrac}
\sectionfont{\fontsize{12}{15}\selectfont}
\subsectionfont{\fontsize{10}{12}\selectfont}
\begin{document}
\title{"Planning and Heuristic Search" \\ Oral Pre-Exam Questions (WS 2018/19)}
\date{}
\maketitle
%\input{oral-questions-1}
\section{What are nodes and edges representing in an OR graph?}
Nodes represent states of a given problem (e.g. board configurations in the 8-puzzle). Edges represent operations (solution steps) that should simplify the problem.


\section{What is a solution path in an OR-graph?}
A path \textit{P} in a state-space graph \textit{G} from node \textit{n} to goal node $\gamma$ in \textit{G}, satisfying given solution constraint,  is called a \textit{solution path} for \textit{n}. 
\subsection*{What is a solution base?}
A path \textit{P} in \textit{G} from \textit{n} to some node \textit{n'} is called \textit{solution base} for \textit{n}.


\section{What are constraint satisfaction problems? What are optimization problems?}
Constraint satisfaction problems are problems where a solution has to fulfill certain constraints and shall be found with minimum search effort. Optimization problems have to find a solution that in addition to the satisfication of constraints also has to stand out amongst all other solutions with respect to a special property.


\section{What is an appropriate representation for infinite graphs?}
The implicit representation is appropriate for infinite graphs as it uses the computable methods \textit{successors()} or \textit{next\_successor()} to determine the direct successors of a node. Its counterpart the explicit representation can only handle finite graphs, as the graph G = (V, E) is explicitly definied.


\section{What is node expansion?}
Applying the function \textit{successors(n)} on a node \textit{n} and thereby creating all direct successors of this node in \textbf{one time step} is called \textit{node expansion}. (All algorithms we considered use node expansion as a basic step, except for the backtracking algorithm)
\subsection*{What is node generation?}
Applying the function \textit{next\_successor(n)} on a node \textit{n} and thereby creating an unseen direct successor (one at a time) is called \textit{node generation}.
\subsection*{What are the states of nodes?} \begin{itemize}
\item generated (living on OPEN)
\item explored (neither on OPEN or CLOSED in A*, since it means \textit{next\_succesor} was applied and there is still at least one unseen node which will be returned by the next call of \textit{next\_succesor}.)
\item expanded (living on CLOSED, except for reopening in A*)
\item unseen
\end{itemize}

\section{What are locally finite graphs? Why do we need them?}
Local finitness means that a node has only a finite number of direct succesors. We do need this property as otherwise we would be stuck in an infinite loop when calling the methods \textit{succesors} or \textit{next\_succesor}.

\section{What is a solution base (differences to solution paths)?}
See question 2. Informally it is the inital part of a possible solution path. Note that we don't necessarily need to find a goal node when following expanding a solution base.

\section{What is an efficient way of representing solution bases?}
Solution bases can be efficiently represented by a \textbf{backpointer path}. A \textit{backpointer} is a reference of a newly generated node, pointing to its parent.

\section{What is the tree Basic-OR-Search maintains?}
The tree maintained by Basic-OR-Search is called a \textit{traversal tree}. It is rooted in the start node \textit{s} and definied by node instances from \textit{G} and edges reverse to the backpointers. Path (\textit{n}, \ldots, \textit{s}), defined by the backpointers of the nodes, is reversed in the traversal tree as (\textit{s}, \ldots, \textit{n}) and is called a backpointer path.

\section{Why is the graph maintained by Basic-OR-Search a tree?}
(A traversal tree is not directly maintained by an algorithm, hence this question is not exact.) It is called traversal tree, because it is a tree-unfolding of a part of the explored subgraph of \textit{G}. Every instance of a node can only have one parent,as it only got one backpointer reference. We don't have cycles because a node in \textit{G} is represented by multiple instances in the traversal tree (if the node got multiple successors that is).

\section{Why is the traversal tree in Basic-OR-Search no subgraph of the OR-graph?}
In general traversal trees are no subgraphs of the  search space graph \textit{G} because traversal trees can contain multiple instances of nodes in \textit{G}.

\section{Are DFS and BFS variants of Basic-OR-Search? Why? / Why not?}
TODO \\
DFS uses a \textit{stack} as OPEN list, hence it uses the LIFO principle.
BFS on the other hand uses a \textit{queue} as OPEN list, hence it uses the FIFO principle.
Both algorithems can be converted into each other by only changing the implementation of the data structure of the OPEN list.
\ldots

\section{Comparison of DFS and BFS: Which algorithm is to be preferred when and why?}
DFS is preferred when:
\begin{itemize}
\item We are given plenty of equivalent solutions.
\item Dead ends can be recognized early, i.e. with considerable look-ahead.
\item There are no cyclic or infinite paths (or at least they can be avoided).
\end{itemize}
BFS is preferred when:
\begin{itemize}
\item We know a solution is not far away from the start node.
\item Solutions are rare and the tree is deep. (Because DFS will take way longer on those graphs.)
\item \ldots 
\end{itemize}
An issue with BFS is that it has to store the explored part of the graph completley in memory. So when a graph is very wide, BFS will need too much memory. However, with BFS we do terminate with a solution if one exists, whereas DFS could follow an endless frutiless path.

\section{Which nodes are stored on OPEN, which nodes on CLOSED?}
Nodes waiting to be expanded are storted on OPEN (this includes both \textit{generated} and \textit{explored} nodes which have not been expanded yet), whereas already expanded nodes are stored on CLOSED.

\section{Why is a function cleanup\_closed() needed in DFS?}
It is needed in order to not run out of memory because of nodes we actually don't need to reference anymore. The function \textit{cleanup\_closed} deletes nodes from the CLOSED list that are no longer required. It is based on the principles that nodes which fulfill the dead end requirement can be discarded without hesitation. If a node \textit{n} is discarded, check if  \textit{n} has any predecessors that are still part of a solution path. A node is part of a solution path if it has a successors on OPEN. Predecessors that are not part of a solution path can be discarded.

\section{What is iterative deepening?}
Increasing the depth-bound of DFS in an outerloop and run DFS with an increased depth-bound value \textit{k} over and over again.

\section{What information sources does the evaluation function f in BF use?}
\begin{enumerate}
\item evaluation of the information given by node \textit{n}
\item estimates of the complexity of the remaining problem at \textit{n} in relation to $\Gamma$
\item evaluations of the explored part \textit{G} of the search space graph
\item domain specific problem solving knowledge
\end{enumerate}

\section{What are the main differences between UCS and BF?}
Cost values in uniform-cost-search (UCS) are stored with the nodes. The OPEN list is organized as a heap, and nodes are explored with respect to the cheapest cost. UCS is also called "cheapest-first-search" and it basically best-first-search (BFS) with some modifications.

\section{What is the difference in the evaluation functions of UCS and BF?}
Uniform-cost-search can only make use of the knowledge from the explored part of the search space graph, the evaluation function \textit{f} in BF can use domain specific knowlege.

\section{What is path discarding?}
For two paths leading to the same node, the one with the higher \textit{f}-value is discarded. (Path discarding is only used in BF* and its successors.)

\section{When using path discarding, is the traversal tree a subgraph of the search space graph?}
TODO \\
\ldots

\section{Why can path discarding be problematic?}
Path discarding is irreversible. It entails the risk of not finding desired solutions. The risk can be eleminated by restricting the evaluation function to be orver-preserving. Path discarding is performed implicitly by maintaining at most one instantiation per node and, therefore, one backpointer per node.

\section{What does node reopening mean?}
Reopening means moving a node from CLOSED to OPEN because the node could be reached with a better \textit{f}-value than the first time it was explored. (Note: Reopening of nodes can be avoided by using a monotonically increasing function \textit{f}, $f(n) \leq f(n')$ for successors $n'$ of $n$.)

\section{What is $C_P(s), C^*(s), \hat{C}_P(s), \hat{C}(s)$ ?}
\subsection{$C_P(s)$}
$C_P$ is a cost function which assigns each solution path $P$ and node $n$ in $P$ a cost value $C_P(n)$. $C_P(s)$  specifies the cost of a solution path  \textit{P} for \textit{s}: 
\begin{align*}
f(\gamma) = C_P(s)
\end{align*}
with $P$ backpointer path of $\gamma$.

\subsection{$C^*(s)$}
$C^*(s) = C^*$ is the optimum solution cost for s. \\
In general: $C^*(n) = $ inf $\{C_P(n) \mid P $ is solution path for $n$ in $G\}$ 

\subsection{$\hat{C}_P(s)$}
$\hat{C}_P(s)$ is the estimated optimum solution cost for $s$ in $G$ based on a solution base $P$ with cost function $C_P(n)$.
$\hat{C}_P(s)$ is optimistic iff $\hat{C}_P(n) \leq  C^*_P(n)$.

\subsection{$\hat{C}(s)$}
$\hat{C}(s)$ is the estimated optimum solution cost for $s$ (no solution base provided. so it searche in all solution bases) \\
Estimated optimum solution cost for node $n$ in $G$: \\
$\hat{C}(n) = $ inf $\{\hat{C}_P(n) \mid P $ is solution base in $G\}$ \\
A solution base $P$ for $s$ with $\hat{C}_P(s) = \hat{C}(s)$ is called most promising solution base (for $s$).

\section{How do we define an evaluation function f by a cost function $\hat{C}$?}
\textbf{UNSURE!} \\
We define $f(n)$ as $\hat{C}_P(n)$ with $P$ backpointer path of $n$: $f(n)$ is a recursive evaluation function.

\section{How do we define recursive cost functions?}
A cost function $C_P$ for a solution path $P$ is called recursive if for each node $n$ in $P$ holds:
$C_P(n) =  
\begin{cases}
F[E(n)] \quad \quad \quad \quad \text{$n$ is leaf in $P$, hence $n$ is goal node.} \\
F[E(n, C_P(n')] \quad \text{$n$ is inner node in $P$ and $n'$ direct succ. of $n$.}
\end{cases}
$

\section{How can we define a function $\hat{C}_P(n)$ for estimated solution cost on basis of recursive cost functions?}
$\hat{C}_P(n) =  
\begin{cases}
c(n) \quad \quad \quad \quad \quad \text{$n$ is leaf in $P$ and $P$ is solution path.} \\
h(n) \quad \quad \quad \quad \quad \text{$n$ is leaf in $P$ but $P$ is no solution path ($n$ in OPEN).} \\
F[E(n), \hat{C}_P(n')] \quad \text{$n$ is inner node in $P$ and $n'$ direct successor of $n$ in $P$.}
\end{cases}
$
\\\\
$E(n)$ denotes local properties of $n$ and $F$ is the so called cost measure. 
$P$ denotes a solution base. The estimated value $h(n)$ is used as if it was correct in the computation of predecessors.

\section{Why can it be an advantage to use recursive cost functions?}
If we use a recursive cost function, we compute tail to front (Face-Value-Principle). This means going all the way down to $\gamma$ and then up again, combining the costs on the way. We have to comnsider the whole Path $P$ when using a recursive cost function. The advantage is then that we simply have to consider the costs of a single node and it's local properties instead of the whole path at once. It is basically a series of local computations.

\section{Can cost functions help to avoid problems in path discarding?}
Idea: Order preserving cost function can help to avoid following the wrong way and thus wrongfuly discarding a optimum paths.
Cost estimations for alternative solution bases must be independet of their shared continuation (S:III-80). $\hat{C}_P(n)$ must be \textit{order-preserving}.

\section{What is the evaluation function used in algorithm A*?}
In A* we use $f(n) = g(n) + h(n)$ as evaluation function.

\section{What is \textit{h} and what is \textit{g} in the evaluation function of algorithm A*?}
$g$ is the sum of edge cost values of the backpointer path from a given node (cost of backpointer path of a given node at some point in time.) \\\\
$h$ is the estimate of the optimum cost of a solution path from a given node. ($h(\gamma) = 0$.)

\section{What is path cost in algorithm A*?}
Sum of edge cost values along that path.

\section{Is the underlying path cost function $\hat{C}_P(s)$ in A* order preserving? Is this only true if we have
negative edge cost values?}
Idea: In A*: $f(n) = \hat{C}_{P_{s-n}}(s) = g_{P_{s-n}}(n) + h(n)$. Evaluation functions $f$ that rely on additive cost measures $F[e, c] = e + c$ are  oder-preserving. So $f(n)$ is order-preserving. It is not only true for negative edge cost values.


\section{Why do we need delayed termination in order to solve optimization problem? (Example?)}
Without delayed termination, BF would return as soon as it found a goal node $\gamma$. With delayed termination we terminate when we select the most promising soloution base from OPEN which happens to be a solution path. Assume we do not use delayed termination and we reached node $n$. The only two goal nodes, $\gamma_1, \gamma_2$ are descendents of $n$. $\gamma_1$ is first explored with a path cost of $100$. BF would immediatley terminate with $\gamma_1$ instead of exploring all possible children of $n$, which could lead to exploring $\gamma_2$ with a much smaller path cost. This is why we need delayed termination. In this case, we would terminate with the cheaper solution path if it happened to be the most promising solution base at this point in time.

\section{What is an optimistic evaluation function?}
An optimistic evaluation function underestimates cost. In particular the true cost of a solution path $P_{s-\gamma}$ extending a solution base $P_{s-n}$ exceeds its $f$-value: $C_{P_{s-\gamma}}(s) \geq f(\gamma)$.

\section{Why do we need optimistic evaluation functions in order to solve optimization problems? (Example?)}
Assume we have a graph with start node $s$ and it's two children $a$ and $b$. There is a goal node $\gamma_a$ and $\gamma_b$, reachable from $a$ and $b$ respectively. Also assume we use a non-optimistic evaluation function. If the $f$-value of $b$ would be $100$ but the actual cost for $P_{s-\gamma_b}$ would be $1$, we would still follow node $a$ if it's $f$-value was for example $5$. We would continue to follow $P_{s-\gamma_a}$ and reach a goal node. The actual cost would then be $5$ which is less than the $1$ we would've paid if we followed node $b$. But $b$ was overestimatic the actual cost, so we never considered the actual cheapest path in this graph.

\section{What is the motivation for specifying Prop(G) for search space graphs?}
Prop($G$):
\begin{enumerate}
\item Search space graph $G$ is an implicitly definied OR-graph.
\item $G$ has only a single start node $s$.
\item $G$ is locally finite.
\item For $G$ a set $\Gamma$ of goal nodes is given. 
\item Each path from $s$ to $\gamma \in \Gamma$ in $G$ is a solution path. In A* *($\gamma$)$= true$, independent of the backpointer path of $\gamma$.
\item Each edge $(n, n')$ in $G$ has non-negative edge-cost $c(n, n')$. The pathcost is the sum of edge cost along that path.
\item $G$ has a positive lower bound $\delta$ of edge costs. $\delta$ is fixed $c(n, n') \geq \delta > 0$ for all $(n, n')$ in $G$.
\item For each node $n$ in $G$ there exists an heuristic estimate $h(n)$ for the cost of the cheapest path from $n$ to $\gamma$. $h(n) \geq 0$, since $h(\gamma) = 0)$ for $\gamma \in \Gamma$.
\end{enumerate}
Motivations for Prop($G$):
\begin{enumerate}
\item 
	\begin{enumerate}[label=(\alph*)] 
	\item \textbf{Implicitly}: Would not be possible to represent infinite graphs with explicit representation.
	\item \textbf{Directed OR-Graph}: We want to use A*, a BF algorithm for OR-graphs.
	\end{enumerate}
\item Simplification.
\item Node expansion. ForEach-Loop would not terminate if $successors$ would return infinite many nodes.
\item Maintenance of structure.
\item No additional constraints on solution paths.
\item 
	\begin{enumerate}[label=(\alph*)] 
	\item \textbf{non-negative cost}: Prune cyclic paths which would corrupt the backpointer structure.
	\item \textbf{sum of edge cost}: Requirement as A* uses additive cost measure. No other calculation is possible.
	\end{enumerate}
\item We get rid of infinite graphs which for example will half the edge cost values all the time, starting with $\nicefrac{1}{2}$ and thus creating an infinite path which we will follow if the other option has edge cost value greather than 1.
\item 
	\begin{enumerate}[label=(\alph*)] 
	\item \textbf{computable}: We want to implement it.
	\item \textbf{$h(n) \geq 0$}: as path cost must not be negative, the estimate should also be non-negative. (Worst case $h(n) = 0$: uninformed search.)
	\item \textbf{$h(\gamma) = 0$}: Simplicity.
	\end{enumerate}
\end{enumerate}

\section{What is the consequence of a positive lower bound of edge cost values for long paths?}
The cost of the path will eventually exceed any given bound if the path is only long enough due to the lower bound of edge cost values $\delta$ .

\section{Is existence of optimum cost solution paths guaranteed for search space graphs with Prop(G)?}
No. However, if there is a solution path, we can argue that there also is an optimum solution path in $G$. But we can not guarantee that there is any solution path in $G$ at all.

\section{What is completeness, what is admissibility for search algorithms?}
\textbf{Completeness}: The algorithm terminates with a solution, if one exists. \\
\textbf{Admissibility}: The algorithm terminates with an optium solution, if a solution exists.

\section{What are the main steps in proving completeness of A*?}
A* is complete for finite graphs $G$ that have $Prop(G)$ (S:V-43). Sketch of proof:
\begin{enumerate}
\item Assume that there is a solution path $P_{s-\gamma}$.
\item At any point before A* terminates there is a shallowest OPEN node on $P_{s-\gamma}$ or all nodes on $P_{s-\gamma}$ have been selected for expansion.
\item If there is a shallowest OPEN node on $P_{s-\gamma}$, A* will not terminate with "fail".
\item If all nodes on $P_{s-\gamma}$ have been selected for expansion, also $\gamma$ has been selected for expansion and A* terminates with solution $\gamma$.
\end{enumerate}

\section{Why can’t we prove termination of A* on infinite graphs?}
Because in the prove of termination of A* we use the fact that the number of cycle-free paths is finite in a finite graph, and therefore it can only exist a finite number of backpointer paths. We can not use this fact for infinite graphs. In other words: A* could follow an infinite path in an infinite graph and never terminate – hence we can not prove termination on infinite graphs. \\
\textbf{ATTENTION}: We can prove completness for infinite graphs, when we assume that there is a solution path. But to prove termination we are not allowed to assume this.

\section{What is a shallowest OPEN node?}
The shallowest OPEN node for a path $P$ is the first node we encounter on that path (starting from $s$) which is on OPEN.

\section{How do shallowest OPEN nodes help proving completeness?}
Due to the shallowest OPEN nodes, we can argue that A* will not terminate with "fail" since there always is an OPEN node which has to be selected for expansion.

\section{What is the additional property of shallowest OPEN nodes on optimum cost paths that is used for proving admissibility of A*?}
We use the lemma of \textit{C*-bounded OPEN nodes} which means that on an optimum cost path there has to be a shallowest OPEN node with $f(n) \leq C^*$. 

\end{document}