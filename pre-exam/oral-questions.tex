\documentclass[12pt, a4paper]{article}
\usepackage{sectsty}
\usepackage{amsmath}
\sectionfont{\fontsize{12}{15}\selectfont}
\subsectionfont{\fontsize{10}{12}\selectfont}
\begin{document}
\title{"Planning and Heuristic Search" \\ Oral Pre-Exam Questions (WS 2018/19)}
\date{}
\maketitle
%\input{oral-questions-1}
\section{What are nodes and edges representing in an OR graph?}
Nodes represent states of a given problem (e.g. board configurations in the 8-puzzle). Edges represent operations (solution steps) that should simplify the problem.


\section{What is a solution path in an OR-graph?}
A path \textit{P} in a state-space graph \textit{G} from node \textit{n} to goal node $\gamma$ in \textit{G}, satisfying given solution constraint,  is called a \textit{solution path} for \textit{n}. 
\subsection*{What is a solution base?}
A path \textit{P} in \textit{G} from \textit{n} to some node \textit{n'} is called \textit{solution base} for \textit{n}.


\section{What are constraint satisfaction problems? What are optimization problems?}
Constraint satisfaction problems are problems where a solution has to fulfill certain constraints and shall be found with minimum search effort. Optimization problems have to find a solution that in addition to the satisfication of constraints also has to stand out amongst all other solutions with respect to a special property.


\section{What is an appropriate representation for infinite graphs?}
The implicit representation is appropriate for infinite graphs as it uses the computable methods \textit{successors()} or \textit{next\_successor()} to determine the direct successors of a node. Its counterpart the explicit representation can only handle finite graphs, as the graph G = (V, E) is explicitly definied.


\section{What is node expansion?}
Applying the function \textit{successors(n)} on a node \textit{n} and thereby creating all direct successors of this node in \textbf{one time step} is called \textit{node expansion}. (All algorithms we considered use node expansion as a basic step, except for the backtracking algorithm)
\subsection*{What is node generation?}
Applying the function \textit{next\_successor(n)} on a node \textit{n} and thereby creating an unseen direct successor (one at a time) is called \textit{node generation}.
\subsection*{What are the states of nodes?} \begin{itemize}
\item generated (living on OPEN)
\item explored (neither on OPEN or CLOSED in A*, since it means \textit{next\_succesor} was applied and there is still at least one unseen node which will be returned by the next call of \textit{next\_succesor}.)
\item expanded (living on CLOSED, except for reopening in A*)
\item unseen
\end{itemize}

\section{What are locally finite graphs? Why do we need them?}
Local finitness means that a node has only a finite number of direct succesors. We do need this property as otherwise we would be stuck in an infinite loop when calling the methods \textit{succesors} or \textit{next\_succesor}.

\section{What is a solution base (differences to solution paths)?}
See question 2. Informally it is the inital part of a possible solution path. Note that we don't necessarily need to find a goal node when following expanding a solution base.

\section{What is an efficient way of representing solution bases?}
Solution bases can be efficiently represented by a \textbf{backpointer path}. A \textit{backpointer} is a reference of a newly generated node, pointing to its parent.

\section{What is the tree Basic-OR-Search maintains?}
The tree maintained by Basic-OR-Search is called a \textit{traversal tree}. It is rooted in the start node \textit{s} and definied by node instances from \textit{G} and edges reverse to the backpointers. Path (\textit{n}, \ldots, \textit{s}), defined by the backpointers of the nodes, is reversed in the traversal tree as (\textit{s}, \ldots, \textit{n}) and is called a backpointer path.

\section{Why is the graph maintained by Basic-OR-Search a tree?}
(A traversal tree is not directly maintained by an algorithm, hence this question is not exact.) It is called traversal tree, because it is a tree-unfolding of a part of the explored subgraph of \textit{G}. Every instance of a node can only have one parent,as it only got one backpointer reference. We don't have cycles because a node in \textit{G} is represented by multiple instances in the traversal tree (if the node got multiple successors that is).

\section{Why is the traversal tree in Basic-OR-Search no subgraph of the OR-graph?}
In general traversal trees are no subgraphs of the  search space graph \textit{G} because traversal trees can contain multiple instances of nodes in \textit{G}.

\section{Are DFS and BFS variants of Basic-OR-Search? Why? / Why not?}
TODO \\
DFS uses a \textit{stack} as OPEN list, hence it uses the LIFO principle.
BFS on the other hand uses a \textit{queue} as OPEN list, hence it uses the FIFO principle.
Both algorithems can be converted into each other by only changing the implementation of the data structure of the OPEN list.
\ldots

\section{Comparison of DFS and BFS: Which algorithm is to be preferred when and why?}
DFS is preferred when:
\begin{itemize}
\item We are given plenty of equivalent solutions.
\item Dead ends can be recognized early, i.e. with considerable look-ahead.
\item There are no cyclic or infinite paths (or at least they can be avoided).
\end{itemize}
BFS is preferred when:
\begin{itemize}
\item We know a solution is not far away from the start node.
\item Solutions are rare and the tree is deep. (Because DFS will take way longer on those graphs.)
\item \ldots 
\end{itemize}
An issue with BFS is that it has to store the explored part of the graph completley in memory. So when a graph is very wide, BFS will need too much memory. However, with BFS we do terminate with a solution if one exists, whereas DFS could follow an endless frutiless path.

\section{Which nodes are stored on OPEN, which nodes on CLOSED?}
Nodes waiting to be expanded are storted on OPEN (this includes both \textit{generated} and \textit{explored} nodes which have not been expanded yet), whereas already expanded nodes are stored on CLOSED.

\section{Why is a function cleanup\_closed() needed in DFS?}
It is needed in order to not run out of memory because of nodes we actually don't need to reference anymore. The function \textit{cleanup\_closed} deletes nodes from the CLOSED list that are no longer required. It is based on the principles that nodes which fulfill the dead end requirement can be discarded without hesitation. If a node \textit{n} is discarded, check if  \textit{n} has any predecessors that are still part of a solution path. A node is part of a solution path if it has a successors on OPEN. Predecessors that are not part of a solution path can be discarded.

\section{What is iterative deepening?}
Increasing the depth-bound of DFS in an outerloop and run DFS with an increased depth-bound value \textit{k} over and over again.

\section{What information sources does the evaluation function f in BF use?}
\begin{enumerate}
\item evaluation of the information given by node \textit{n}
\item estimates of the complexity of the remaining problem at \textit{n} in relation to $\Gamma$
\item evaluations of the explored part \textit{G} of the search space graph
\item domain specific problem solving knowledge
\end{enumerate}

\section{What are the main differences between UCS and BF?}
Cost values in uniform-cost-search (UCS) are stored with the nodes. The OPEN list is organized as a heap, and nodes are explored with respect to the cheapest cost. UCS is also called "cheapest-first-search" and it basically best-first-search (BFS) with some modifications.

\section{What is the difference in the evaluation functions of UCS and BF?}
Uniform-cost-search can only make use of the knowledge from the explored part of the search space graph, the evaluation function \textit{f} in BF can use domain specific knowlege.

\section{What is path discarding?}
For two paths leading to the same node, the one with the higher \textit{f}-value is discarded. (Path discarding is only used in BF* and its successors.)

\section{When using path discarding, is the traversal tree a subgraph of the search space graph?}
TODO \\
\ldots

\section{Why can path discarding be problematic?}
Path discarding is irreversible. It entails the risk of not finding desired solutions. The risk can be eleminated by restricting the evaluation function to be orver-preserving. Path discarding is performed implicitly by maintaining at most one instantiation per node and, therefore, one backpointer per node.

\section{What does node reopening mean?}
Reopening means moving a node from CLOSED to OPEN because the node could be reached with a better \textit{f}-value than the first time it was explored. (Note: Reopening of nodes can be avoided by using a monotonically increasing function \textit{f}, $f(n) \leq f(n')$ for successors $n'$ of $n$.)

\section{What is $C_P(s), C^*(s), \hat{C}_P(s), \hat{C}(s)$ ?}
\subsection{$C_P(s)$}
$C_P$ is a cost function which assigns each solution path $P$ and node $n$ in $P$ a cost value $C_P(n)$. $C_P(s)$  specifies the cost of a solution path  \textit{P} for \textit{s}: 
\begin{align*}
f(\gamma) = C_P(s)
\end{align*}
with $P$ backpointer path of $\gamma$.

\subsection{$C^*(s)$}
$C^*(s) = C^*$ is the optimum solution cost for s. \\
In general: $C^*(n) = $ inf $\{C_P(n) \mid P $ is solution path for $n$ in $G\}$ 

\subsection{$\hat{C}_P(s)$}
$\hat{C}_P(s)$ is the estimated optimum solution cost for $s$ in $G$ based on a solution base $P$ with cost function $C_P(n)$.
$\hat{C}_P(s)$ is optimistic iff $\hat{C}_P(n) \leq  C^*_P(n)$.

\subsection{$\hat{C}(s)$}
$\hat{C}(s)$ is the estimated optimum solution cost for $s$ (no solution base provided. so it searche in all solution bases) \\
Estimated optimum solution cost for node $n$ in $G$: \\
$\hat{C}(n) = $ inf $\{\hat{C}_P(n) \mid P $ is solution base in $G\}$ \\
A solution base $P$ for $s$ with $\hat{C}_P(s) = \hat{C}(s)$ is called most promising solution base (for $s$).

\section{How do we define an evaluation function f by a cost function $\hat{C}$?}
\textbf{UNSURE!} \\
We define $f(n)$ as $\hat{C}_P(n)$ with $P$ backpointer path of $n$: $f(n)$ is a recursive evaluation function.

\section{How do we define recursive cost functions?}
A cost function $C_P$ for a solution path $P$ is called recursive if for each node $n$ in $P$ holds:
$C_P(n) =  
\begin{cases}
F[E(n)] \quad \quad \quad \quad \text{$n$ is leaf in $P$, hence $n$ is goal node.} \\
F[E(n, C_P(n')] \quad \text{$n$ is inner node in $P$ and $n'$ direct succ. of $n$.}
\end{cases}
$

\section{How can we define a function $\hat{C}_P(n)$ for estimated solution cost on basis of recursive cost functions?}
$\hat{C}_P(n) =  
\begin{cases}
c(n) \quad \quad \quad \quad \quad \text{$n$ is leaf in $P$ and $P$ is solution path.} \\
h(n) \quad \quad \quad \quad \quad \text{$n$ is leaf in $P$ but $P$ is no solution path ($n$ in OPEN).} \\
F[E(n), \hat{C}_P(n')] \quad \text{$n$ is inner node in $P$ and $n'$ direct successor of $n$ in $P$.}
\end{cases}
$
\\\\
$E(n)$ denotes local properties of $n$ and $F$ is the so called cost measure. 
$P$ denotes a solution base. The estimated value $h(n)$ is used as if it was correct in the computation of predecessors.

\section{Why can it be an advantage to use recursive cost functions?}
If we use a recursive cost function, we compute tail to front (Face-Value-Principle). This means going all the way down to $\gamma$ and then up again, combining the costs on the way. We have to comnsider the whole Path $P$ when using a recursive cost function. The advantage is then that we simply have to consider the costs of a single node and it's local properties instead of the whole path at once. It is basically a series of local computations.

\section{Can cost functions help to avoid problems in path discarding?}
Idea: Order preserving cost function can help to avoid following the wrong way and thus wrongfuly discarding a optimum paths.
Cost estimations for alternative solution bases must be independet of their shared continuation (S:III-80). $\hat{C}_P(n)$ must be \textit{order-preserving}.


\end{document}